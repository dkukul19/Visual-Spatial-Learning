{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f55bd5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pyyaml==5.1\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git@v0.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f369f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget http://images.cocodataset.org/zips/val2014.zip\n",
    "!unzip val2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "51c6133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip\n",
    "!unzip v2_Annotations_Val_mscoco.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b1c03f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade tensorflow\n",
    "!pip install --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514e7dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('v2_OpenEnded_mscoco_val2014_questions.json') as f:\\n    q = json.load(f)\\n\\nwith open('v2_mscoco_val2014_annotations.json') as f:\\n    a = json.load(f)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "\n",
    "\n",
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.structures.image_list import ImageList\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers\n",
    "from detectron2.structures.boxes import Boxes\n",
    "from detectron2.layers import nms\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "with open('v2_OpenEnded_mscoco_val2014_questions.json') as f:\n",
    "    q = json.load(f)\n",
    "\n",
    "with open('v2_mscoco_val2014_annotations.json') as f:\n",
    "    a = json.load(f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "998d4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05824810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#000000000142.jpg\",\"caption\":\"The banana is part of the sandwich. label 1\n",
    "#000000000370.jpg\",\"caption\":\"The person is touching the broccoli.\" label 1\n",
    "#000000000397.jpg\",\"caption\":\"The pizza is over the dining table.\" label 1\n",
    "#000000002570.jpg\",\"caption\":\"The donut is in front of the laptop.\" label 1\n",
    "#000000003862.jpg\",\"caption\":\"The dog is on top of the bench.\",\"label\":0,\n",
    "#\"photos/bag_in.png\",\"photos/monkey_out.png\",\"photos/monkeyin.png\",\"photos/applein_orangeout.png\",\"photos/spatula_out.png\"\n",
    "\n",
    "#\"photos/orange_out.png\",\n",
    "image_names = [\"images/000000003862.jpg\"]\n",
    "\n",
    "images = [cv2.cvtColor(plt.imread(n),cv2.COLOR_RGB2BGR) for n in image_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aaf6e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "\n",
    "def load_config_and_model_weights(cfg_path):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(cfg_path))\n",
    "\n",
    "    # ROI HEADS SCORE THRESHOLD\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "    # Comment the next line if you're using 'cuda'\n",
    "    cfg['MODEL']['DEVICE']='cpu'\n",
    "\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(cfg_path)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "cfg = load_config_and_model_weights(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1c8892af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(cfg):\n",
    "    # build model\n",
    "    model = build_model(cfg)\n",
    "\n",
    "    # load weights\n",
    "    checkpointer = DetectionCheckpointer(model)\n",
    "    checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "    # eval mode\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = get_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "011c2569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_inputs(cfg, img_list):\n",
    "    # Resizing the image according to the configuration\n",
    "    transform_gen = T.ResizeShortestEdge(\n",
    "                [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "            )\n",
    "    img_list = [transform_gen.get_transform(img).apply_image(img) for img in img_list]\n",
    "\n",
    "    # Convert to C,H,W format\n",
    "    convert_to_tensor = lambda x: torch.Tensor(x.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "    batched_inputs = [{\"image\":convert_to_tensor(img), \"height\": img.shape[0], \"width\": img.shape[1]} for img in img_list]\n",
    "\n",
    "    # Normalizing the image\n",
    "    num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
    "    pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1)\n",
    "    pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1)\n",
    "    normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
    "    images = [normalizer(x[\"image\"]) for x in batched_inputs]\n",
    "\n",
    "    # Convert to ImageList\n",
    "    images =  ImageList.from_tensors(images,model.backbone.size_divisibility)\n",
    "    \n",
    "    return images, batched_inputs\n",
    "\n",
    "images, batched_inputs = prepare_image_inputs(cfg, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "281731dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model, images):\n",
    "    features = model.backbone(images.tensor)\n",
    "    return features\n",
    "\n",
    "features = get_features(model, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d2226614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.show()\\nfor key in features.keys():\\n    print(key)\\n    print(features[key].shape)\\n    #plt.imshow(features[key][1,0,:,:].squeeze().detach().numpy(), cmap=\\'jet\\')\\n    plt.show()\\n\\nprint(len(features[\"p4\"]))\\nprint(features[\"p4\"])\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.imshow(cv2.resize(img2, (images.tensor.shape[-2:][::-1])))\n",
    "\"\"\"\n",
    "plt.show()\n",
    "for key in features.keys():\n",
    "    print(key)\n",
    "    print(features[key].shape)\n",
    "    #plt.imshow(features[key][1,0,:,:].squeeze().detach().numpy(), cmap='jet')\n",
    "    plt.show()\n",
    "\n",
    "print(len(features[\"p4\"]))\n",
    "print(features[\"p4\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f0435fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proposals(model, images, features):\n",
    "    proposals, _ = model.proposal_generator(images, features)\n",
    "    return proposals\n",
    "\n",
    "proposals = get_proposals(model, images, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ddd57d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_features(model, features, proposals):\n",
    "    features_list = [features[f] for f in ['p2', 'p3', 'p4', 'p5']]\n",
    "    box_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "    box_features = model.roi_heads.box_head.flatten(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc1(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc_relu1(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc2(box_features)\n",
    "\n",
    "    box_features = box_features.reshape(1, 1000, 1024) # depends on your config and batch size\n",
    "    return box_features, features_list\n",
    "\n",
    "box_features, features_list = get_box_features(model, features, proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c9a8bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_logits(model, features_list, proposals):\n",
    "    cls_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "    cls_features = model.roi_heads.box_head(cls_features)\n",
    "    pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(cls_features)\n",
    "    return pred_class_logits, pred_proposal_deltas\n",
    "\n",
    "pred_class_logits, pred_proposal_deltas = get_prediction_logits(model, features_list, proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7fa7ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRCNNOutputs:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        box2box_transform,\n",
    "        pred_class_logits,\n",
    "        pred_proposal_deltas,\n",
    "        proposals,\n",
    "        smooth_l1_beta=0.0,\n",
    "        box_reg_loss_type=\"smooth_l1\",\n",
    "    ):\n",
    "\n",
    "        self.box2box_transform = box2box_transform\n",
    "        self.num_preds_per_image = [len(p) for p in proposals]\n",
    "        self.pred_class_logits = pred_class_logits\n",
    "        self.pred_proposal_deltas = pred_proposal_deltas\n",
    "        self.smooth_l1_beta = smooth_l1_beta\n",
    "        self.box_reg_loss_type = box_reg_loss_type\n",
    "\n",
    "        self.image_shapes = [x.image_size for x in proposals]\n",
    "\n",
    "        #concatenation\n",
    "        if len(proposals):\n",
    "            box_type = type(proposals[0].proposal_boxes)\n",
    "            # cat(..., dim=0) concatenates over all images in the batch\n",
    "            self.proposals = box_type.cat([p.proposal_boxes for p in proposals])\n",
    "            assert (\n",
    "                not self.proposals.tensor.requires_grad\n",
    "            ), \"Proposals should not require gradients!\"\n",
    "\n",
    "            if proposals[0].has(\"gt_classes\"):\n",
    "                self.gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n",
    "                \n",
    "                gt_boxes = [\n",
    "                    p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes for p in proposals\n",
    "                ]\n",
    "                self.gt_boxes = box_type.cat(gt_boxes)\n",
    "        else:\n",
    "            self.proposals = Boxes(torch.zeros(0, 4, device=self.pred_proposal_deltas.device))\n",
    "        self._no_instances = len(self.proposals) == 0  # no instances found\n",
    "        \n",
    "\n",
    "    def predict_boxes(self):\n",
    "        \"\"\"\n",
    "        Deprecated\n",
    "        \"\"\"\n",
    "        pred = self.box2box_transform.apply_deltas(self.pred_proposal_deltas, self.proposals.tensor)\n",
    "        return pred.split(self.num_preds_per_image, dim=0)\n",
    "\n",
    "    def predict_probs(self):\n",
    "        \"\"\"\n",
    "        Deprecated\n",
    "        \"\"\"\n",
    "        probs = torch.nn.functional.softmax(self.pred_class_logits, dim=-1)\n",
    "        return probs.split(self.num_preds_per_image, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9901664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_scores(cfg, pred_class_logits, pred_proposal_deltas):\n",
    "    box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
    "    smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
    "\n",
    "    outputs = FastRCNNOutputs(\n",
    "        box2box_transform,\n",
    "        pred_class_logits,\n",
    "        pred_proposal_deltas,\n",
    "        proposals,\n",
    "        smooth_l1_beta,\n",
    "    )\n",
    "    \n",
    "\n",
    "    boxes = outputs.predict_boxes()\n",
    "    scores = outputs.predict_probs()\n",
    "    image_shapes = outputs.image_shapes\n",
    "\n",
    "    return boxes, scores, image_shapes\n",
    "\n",
    "boxes, scores, image_shapes = get_box_scores(cfg, pred_class_logits, pred_proposal_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d84bae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_boxes(boxes, batched_inputs, image_size):\n",
    "    proposal_boxes = boxes.reshape(-1, 4).clone()\n",
    "    scale_x, scale_y = (batched_inputs[\"width\"] / image_size[1], batched_inputs[\"height\"] / image_size[0])\n",
    "    output_boxes = Boxes(proposal_boxes)\n",
    "\n",
    "    output_boxes.scale(scale_x, scale_y)\n",
    "    output_boxes.clip(image_size)\n",
    "\n",
    "    return output_boxes\n",
    "\n",
    "output_boxes = [get_output_boxes(boxes[i], batched_inputs[i], proposals[i].image_size) for i in range(len(proposals))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c7302575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_boxes(cfg, output_boxes, scores):\n",
    "    test_score_thresh = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "    test_nms_thresh = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
    "    cls_prob = scores.detach()\n",
    "    cls_boxes = output_boxes.tensor.detach().reshape(1000,80,4)\n",
    "    max_conf = torch.zeros((cls_boxes.shape[0]))\n",
    "    for cls_ind in range(0, cls_prob.shape[1]-1):\n",
    "        cls_scores = cls_prob[:, cls_ind+1]\n",
    "        det_boxes = cls_boxes[:,cls_ind,:]\n",
    "        keep = np.array(nms(det_boxes, cls_scores, test_nms_thresh))\n",
    "        max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep], cls_scores[keep], max_conf[keep])\n",
    "    keep_boxes = torch.where(max_conf >= test_score_thresh)[0]\n",
    "    return keep_boxes, max_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "249801e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [select_boxes(cfg, output_boxes[i], scores[i]) for i in range(len(scores))]\n",
    "keep_boxes, max_conf = [],[]\n",
    "for keep_box, mx_conf in temp:\n",
    "    keep_boxes.append(keep_box)\n",
    "    max_conf.append(mx_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d66d81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_BOXES=10\n",
    "MAX_BOXES=100\n",
    "def filter_boxes(keep_boxes, max_conf, min_boxes, max_boxes):\n",
    "    if len(keep_boxes) < min_boxes:\n",
    "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:min_boxes]\n",
    "    elif len(keep_boxes) > max_boxes:\n",
    "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:max_boxes]\n",
    "    return keep_boxes\n",
    "\n",
    "keep_boxes = [filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a148541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visual_embeds(box_features, keep_boxes):\n",
    "    return box_features[keep_boxes.copy()]\n",
    "\n",
    "#visual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]\n",
    "#print(\"and visual embeds, finally:\",visual_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8d6240a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import urllib\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa8290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.16s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=119.32s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.27s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertForVisualReasoning: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing VisualBertForVisualReasoning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBertForVisualReasoning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualBertForVisualReasoning were not initialized from the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre and are newly initialized: ['cls.weight', 'cls.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anns: [{'segmentation': [[374.5, 374.31, 375.15, 372.95, 378.06, 372.5, 379.48, 373.28, 380.13, 375.54, 380.77, 378, 380.38, 380.72, 379.87, 382.91], [392.67, 375.28, 394.29, 374.05, 395.78, 373.02, 397.46, 371.72, 397.85, 370.88, 399.27, 371.47, 398.88, 373.15, 396.88, 374.57, 395, 375.22, 393.32, 375.61]], 'num_keypoints': 0, 'area': 45.7978, 'iscrowd': 0, 'keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'image_id': 191360, 'bbox': [374.5, 370.88, 24.77, 12.03], 'category_id': 1, 'id': 2030274}]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'COCOVisualReasoningDataset' object has no attribute 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    113\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;66;03m# Get the inputs and labels\u001b[39;00m\n\u001b[0;32m    117\u001b[0m         inputs, targets, captions, keypoints \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:517\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 517\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    520\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    556\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    559\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[8], line 83\u001b[0m, in \u001b[0;36mCOCOVisualReasoningDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     81\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m#tokenized = texte.apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True)))\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m caption \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mencode(caption)\u001b[38;5;241m.\u001b[39mids, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(target)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Convert the annotations to tensors\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m#return img, target, caption, keypoints\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'COCOVisualReasoningDataset' object has no attribute 'tokenizer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "from torchvision.transforms import transforms\n",
    "from transformers import BertTokenizer, VisualBertForVisualReasoning\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "# Define the data directory and annotation file paths\n",
    "data_dir = 'train2017/'\n",
    "ann_file_instances = 'annotations/instances_train2017.json'\n",
    "ann_file_captions = 'annotations/captions_train2017.json'\n",
    "ann_file_keypoints = COCO('annotations/person_keypoints_train2017.json')\n",
    "\n",
    "\n",
    "ann_ids = ann_file_keypoints.getAnnIds()\n",
    "anns = ann_file_keypoints.loadAnns(ann_ids)\n",
    "\n",
    "coco_keypoints = []\n",
    "for ann in anns:\n",
    "    if ann['iscrowd'] == 1:\n",
    "        continue\n",
    "    kp = ann['keypoints']\n",
    "    # Convert the keypoints to a list of (x, y, v) tuples\n",
    "    coco_keypoints.append([(kp[i], kp[i+1], kp[i+2]) for i in range(0, len(kp), 3)])\n",
    "    \n",
    "# Load the COCO dataset using the COCODataset class\n",
    "coco_instances = torchvision.datasets.CocoDetection(root=data_dir, annFile=ann_file_instances)\n",
    "coco_captions = torchvision.datasets.CocoCaptions(root=data_dir, annFile=ann_file_captions)\n",
    "#coco_keypoints = torchvision.datasets.CocoKeyPoints(root=data_dir, annFile=ann_file_keypoints)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "\n",
    "class COCOVisualReasoningDataset(data.Dataset):\n",
    "    def __init__(self, coco_instances, coco_captions, coco_keypoints, transform=None):\n",
    "        self.coco_instances = coco_instances\n",
    "        self.coco_captions = coco_captions\n",
    "        self.coco_keypoints = coco_keypoints\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get the image and annotations for the given index\n",
    "        img, target = self.coco_instances[index]\n",
    "        caption = self.coco_captions[index][0]\n",
    "        keypoints = self.coco_keypoints[index]\n",
    "        \n",
    "        ann_ids = ann_file_keypoints.getAnnIds(target[0][\"image_id\"])\n",
    "        anns = ann_file_keypoints.loadAnns(ann_ids)\n",
    "\n",
    "        #ann['category_id']\n",
    "\n",
    "        target = {\n",
    "            'boxes': [ann['bbox'] for ann in anns],\n",
    "            'labels': [ann['category_id']-1 for ann in anns],\n",
    "            'keypoints': [],\n",
    "        }\n",
    "        for ann in anns:\n",
    "            if ann['iscrowd'] == 1:\n",
    "                continue\n",
    "            kp = ann['keypoints']\n",
    "            target['keypoints'].append([(kp[i], kp[i+1], kp[i+2]) for i in range(0, len(kp), 3)])\n",
    "            \n",
    "        #caption = self.coco_caption.anns[self.caption_ids[index]]['caption']\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        boxes = torch.tensor(target['boxes'], dtype=torch.float32)\n",
    "        labels = torch.tensor(target['labels'], dtype=torch.int64)\n",
    "        #labels = torch.tensor(1).unsqueeze(0)\n",
    "        keypoints = torch.tensor(target['keypoints'], dtype=torch.float32)\n",
    "        #tokenized = texte.apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True)))\n",
    "        caption = torch.tensor(tokenizer.encode(caption).ids, dtype=torch.int64)\n",
    "\n",
    "\n",
    "        #return img, target, caption, keypoints\n",
    "        return img, boxes, labels, keypoints, caption\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.coco_instances)\n",
    "\n",
    "# Define the training parameters\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "dataset = COCOVisualReasoningDataset(coco_instances, coco_captions, coco_keypoints, transform=transform)\n",
    "dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "model = VisualBertForVisualReasoning.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, targets, captions, keypoints = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs, captions=captions, object_features=targets, object_locations=keypoints)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            print(running_loss)\n",
    "            print(loss.item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "260ebb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertForVisualReasoning: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing VisualBertForVisualReasoning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBertForVisualReasoning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualBertForVisualReasoning were not initialized from the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre and are newly initialized: ['cls.weight', 'cls.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertForVisualReasoning: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing VisualBertForVisualReasoning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBertForVisualReasoning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualBertForVisualReasoning were not initialized from the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre and are newly initialized: ['cls.weight', 'cls.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertForVisualReasoning: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing VisualBertForVisualReasoning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBertForVisualReasoning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualBertForVisualReasoning were not initialized from the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre and are newly initialized: ['cls.weight', 'cls.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertForVisualReasoning: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing VisualBertForVisualReasoning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBertForVisualReasoning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualBertForVisualReasoning were not initialized from the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre and are newly initialized: ['cls.weight', 'cls.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertForVisualReasoning: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing VisualBertForVisualReasoning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBertForVisualReasoning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualBertForVisualReasoning were not initialized from the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre and are newly initialized: ['cls.weight', 'cls.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2294, grad_fn=<NllLossBackward>) tensor([[0.2050, 0.7950]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.5751, grad_fn=<NllLossBackward>) tensor([[0.4374, 0.5626]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.6456, grad_fn=<NllLossBackward>) tensor([[0.4756, 0.5244]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.6695, grad_fn=<NllLossBackward>) tensor([[0.4880, 0.5120]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.7918, grad_fn=<NllLossBackward>) tensor([[0.5470, 0.4530]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, VisualBertForVisualReasoning\n",
    "import torch\n",
    "\n",
    "\n",
    "for k in range(5):\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = VisualBertForVisualReasoning.from_pretrained(\"uclanlp/visualbert-nlvr2-coco-pre\")\n",
    "\n",
    "    visual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]\n",
    "\n",
    "    visual_embeds = torch.stack(visual_embeds)\n",
    "\n",
    "    for i in range(len(images)):\n",
    "\n",
    "        text = \"Is the dog is on under the bench?\" #get text from captions\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "        #torch ones\n",
    "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
    "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
    "\n",
    "\n",
    "        inputs.update(\n",
    "            {\n",
    "                \"visual_embeds\": visual_embeds[i:i+1],\n",
    "                \"visual_token_type_ids\": visual_token_type_ids[i:i+1],\n",
    "                \"visual_attention_mask\": visual_attention_mask[i:i+1],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #no need to use unsqueeze for visual embeds because its already a list holding lists\n",
    "        labels = torch.tensor(1).unsqueeze(0)  # Batch size 1, Num choices 2\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        #print(\"labels:\",labels)\n",
    "        loss = outputs.loss\n",
    "        scores = outputs.logits\n",
    "        out = F.softmax(scores,dim = 1)\n",
    "        \n",
    "        losses[loss]=out\n",
    "    \n",
    "#compare all losses\n",
    "for key in sorted(losses):\n",
    "    print (key, losses[key])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f52068a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoTokenizer, VisualBertForQuestionAnswering\\nimport torch\\n\\nvisual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]\\n\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\nmodel = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")\\n\\ntext = \"Where is the monkey?\"\\ninputs = tokenizer(text, return_tensors=\"pt\")\\n#visual_embeds = get_visual_embeds(image).unsqueeze(0)\\nvisual_embeds = torch.stack(visual_embeds)\\nvisual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\\nvisual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\\n\\ninputs.update(\\n    {\\n        \"visual_embeds\": visual_embeds[0:1],\\n        \"visual_token_type_ids\": visual_token_type_ids[0:1],\\n        \"visual_attention_mask\": visual_attention_mask[0:1],\\n    }\\n)\\n\\nlabels = torch.tensor([[0.0, 1.0]]).unsqueeze(0)  # Batch size 1, Num choices 2\\noutputs = model(**inputs, labels=labels)\\nprint(outputs)\\n#print(f\"output: {out.shape}\")\\nloss = outputs.loss\\nprint(loss)\\nscores = outputs.logits\\nprint(scores)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoTokenizer, VisualBertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "visual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")\n",
    "\n",
    "text = \"Where is the monkey?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "#visual_embeds = get_visual_embeds(image).unsqueeze(0)\n",
    "visual_embeds = torch.stack(visual_embeds)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
    "\n",
    "inputs.update(\n",
    "    {\n",
    "        \"visual_embeds\": visual_embeds[0:1],\n",
    "        \"visual_token_type_ids\": visual_token_type_ids[0:1],\n",
    "        \"visual_attention_mask\": visual_attention_mask[0:1],\n",
    "    }\n",
    ")\n",
    "\n",
    "labels = torch.tensor([[0.0, 1.0]]).unsqueeze(0)  # Batch size 1, Num choices 2\n",
    "outputs = model(**inputs, labels=labels)\n",
    "print(outputs)\n",
    "#print(f\"output: {out.shape}\")\n",
    "loss = outputs.loss\n",
    "print(loss)\n",
    "scores = outputs.logits\n",
    "print(scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(outputs[\"instances\"].pred_classes)\n",
    "#print(outputs[\"instances\"].pred_boxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
